from celery import task
from django.conf import settings

from treeherder.model.derived import JobsModel
from treeherder.model.models import Datasource, Repository

@task(name='process-objects')
def process_objects(limit=None):
    """
    Process a number of objects from the objectstore
    and load them to the jobs store
    """
    # default limit to 100
    limit = limit or 100
    for ds in Datasource.objects.all():
        jm = JobsModel(ds.project)
        try:
            jm.process_objects(limit)
        finally:
            jm.disconnect()

# Run a maximum of 1 per hour
@task(name='cycle-data', rate_limit='1/h')
def cycle_data(max_iterations=50, debug=False):

    projects = Repository.objects.all().values_list('name', flat=True)

    for project in projects:

        jm = JobsModel(project)

        sql_targets = {}

        if debug:
            print "Cycling Database: {0}".format(project)

        cycle_iterations = max_iterations

        while cycle_iterations > 0:

            sql_targets = jm.cycle_data(sql_targets)

            if debug:
                print "Iterations: {0}".format(str(cycle_iterations))
                print "sql_targets"
                print sql_targets

            cycle_iterations -= 1

            # No more items to delete
            if sql_targets['total_count'] == 0:
                cycle_iterations = 0

        jm.disconnect()

@task(name='calculate-eta', rate_limit='1/h')
def calculate_eta(sample_window_seconds=21600, debug=False):

    projects = Repository.objects.all().values_list('name', flat=True)

    for project in projects:

        jm = JobsModel(project)

        jm.calculate_eta(sample_window_seconds, debug)

        jm.disconnect()

@task(name='populate-performance-series')
def populate_performance_series(project, series_type, series_data):

    jm = JobsModel(project)
    for t_range in settings.TREEHERDER_PERF_SERIES_TIME_RANGES:
        for signature in series_data:
            jm.store_performance_series(
                t_range['seconds'], series_type, signature,
                series_data[signature]
            )
    jm.disconnect()

from exchanges import TreeherderPublisher
from pulse_publisher import load_schemas
import os

# Load schemas for validation of messages published on pulse
source_folder = os.path.dirname(os.path.realpath(__file__))
schema_folder = os.path.join(source_folder, '..', '..', 'schemas')
schemas = load_schemas(schema_folder)

# Find an appropriate namespace to publish to under, this will be removed when
# pulse guardian has multi-user, then we'll just not provide the namespace
# property and the settings property will provide client_id and access_token
# from which client_id will be the namespace.
namespace = 'treeherder-local'
if 'treeherder.allizom.org' in settings.SITE_URL:
    namespace = 'treeherder-staging'
elif 'treeherder.mozilla.org' in settings.SITE_URL:
    namespace = 'treeherder'

publisher = TreeherderPublisher(
    client_id       = 'public',
    access_token    = 'public',
    schemas         = schemas,
    namespace       = namespace
)

@task(name='publish-to-pulse')
def publish_to_pulse(project, ids, data_type):
    jm = JobsModel(project)

    # Publish messages with new result-sets
    if data_type == 'result_set':
        # Get appropriate data for data_type
        # using the ids provided
        for entry in jm.get_result_set_list_by_ids(ids):
            # Don't expose these properties, they are internal, at least that's
            # what I think without documentation I have no clue... what any of
            # this is
            del entry['revisions']      # Not really internal, but too big
            del entry['repository_id']

            # Set required properties
            entry['version'] = 1
            entry['project'] = project
            # Property revision_hash should already be there, I suspect it is the
            # result-set identifier...

            # publish the data to pulse
            publisher.new_result_set(
                message         = entry,
                revision_hash   = entry['revision_hash'],
                project         = project
            )

        # Basically, I have no idea what context this runs and was inherently
        # unable to make kombu with or without pyamqp, etc. confirm-publish,
        # so we're stuck with this super ugly hack where we just close the
        # connection so that if the process context is destroyed then at least
        # messages will still get published... Well, assuming nothing goes
        # wrong, because we're not using confirm channels for publishing...
        publisher.connection.release()

    jm.disconnect()
